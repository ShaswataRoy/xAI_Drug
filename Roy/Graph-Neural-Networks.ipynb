{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning on Graphs with Message Passing Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point in the Straight Dope, we've seen a wide variety of different types of data fed as input to our models.\n",
    "\n",
    "We started with linear regression models and MLPs, which take simple, 1-dimensional vectors of real numbers as input.  Then we met CNNs, which take images represented as 3-dimensional tensors as input.  Next we saw how RNNs can take sequence data, like time-series or natural-language sentences, or really anything we can represent as a sequence of tensors, as input.  And we even saw how to consume tree-structured data, like a parse tree of a natural-language sentence, using a Tree LSTM.\n",
    "\n",
    "In this chapter we'll see how to build models to handle yet another type of data: graph-structured data. We'll learn how to build Message Passing Neural Networks (MPNNs), which are a class of deep model that can take arbitrary graphs as input.\n",
    "\n",
    "**Wait, \"graphs\"?**\n",
    "\n",
    "When I say \"graph\", I mean that word the way a mathematician means it.  [Wikipedia explains the concept well][1], if you're not familiar.  Going forward I'll assume we're familiar with graph-lingo like \"directed edge\" and \"adjacency matrix\", so take a gander at that link if you need to.\n",
    "\n",
    "**So what exactly does \"taking graphs as input\" mean?**\n",
    "\n",
    "Good question!  Reading papers or blogs about this topic can be confusing, since (at least) two distinct learning scenarios both go by the name \"learning on graphs\":\n",
    "1. *We're trying to learn a model whose inputs are arbitrary graphs.*  Our dataset consists of (graph, label) pairs.  E.g. predicting the pharmacological activity of a molecule based on how its atoms are connected.\n",
    "    \n",
    "2. *We're trying to learn a model whose inputs are vertices in some graph.*  Our dataset is one big graph whose vertices are datapoints with edges between them, some labeled, some unlabeled.  E.g. predicting the impact factor of an article given a bag-of-words representation of the article and edges connecting it to its references.\n",
    "\n",
    "In this chapter, we're focusing only on scenario 1, but MPNNs can be used for scenario 2 as well.\n",
    "\n",
    "**Aren't sequences and trees just special types of graphs?  We already know models that handle those. (RNNs and Tree-RNNs.)**\n",
    "\n",
    "Yes they are!  In fact you can (and people do) even think of images as graphs where each pixel is a vertex with edges to all its adjacent pixels.  But MPNNs can operate on *any* type of graph: directed or not, cyclic or not, etc.  Be careful though: MPNNs likely won't perform as well on sequences, trees, or images as models designed specifically for these data types will.\n",
    "\n",
    "**But can't you basically represent anything as a graph if you try hard enough?**\n",
    "\n",
    "Yeah, that's partly why graphs are ubiquitous in math and computer science.  They're a super general concept.  \n",
    "\n",
    "This generality should make us veeeery suspicious that deep learning on graphs won't work as consistently well as, say, deep learning on real-world images does.  If it did, we could use deep networks to reason about nearly anything, and that would smell like a free lunch.\n",
    "\n",
    "But MPNNs are still worth learning about.  They're the best tool we have at the moment for understanding graph-structured data, and they're a hot area of research.\n",
    "\n",
    "[1]:https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Message Passing Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Message Passing Neural Networks were introduced in [this paper](https://arxiv.org/pdf/1704.01212.pdf).  MPNNs are actually a family of models rather than a specific implementation, like how RNNs are a general model family, one implementation of which is an LSTM.  We'll first go over the general MPNN idea and then build a specific implementation.\n",
    "\n",
    "### The Setup\n",
    "\n",
    "We've got a dataset of `(graph, label)` pairs.  In each graph, each vertex $v$ has associated features $x_v$, and each edge has features $e_{vw}$.  For simplicity of explanation we'll assume each graph is undirected, but once you understand MPNNs it's easy to see how to extend them to directed graphs or multigraphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model\n",
    "\n",
    "The goal of an MPNN is to take in a `graph` and output the correct `label`.  They do this by the following procedure:\n",
    "1. Initialize a \"hidden state\" $h_v^0$ for each vertex $v$ in the graph as a function of the vertex's features: $$h_v^0 = \\text{init_hidden}(x_v).$$\n",
    "2. For each round $t$ out of $T$ total rounds:\n",
    "    3. Each vertex $v$ receives a \"message\" $m_v^{t+1}$, which is the sum of messages passed by $v$'s neighbors as functions of their current hidden states and the edge features: $$m_v^{t+1} = \\sum_{w \\in \\text{neighbors of }v} M_t(h_v^t, h_w^t, e_{vw}).$$\n",
    "    4. Each vertex $v$ updates its hidden state as a function of the message it received: $$h_v^{t+1} = U_t(h_v^t, m_v^{t+1}).$$\n",
    "5. The output is computed as the \"readout\" function of all the hidden states: $$\\hat{y} = R_t(\\{h_v^T \\vert v \\text{ is in the graph} \\}).$$\n",
    "\n",
    "Here's an base class for any type of MPNN that encapsulates this procedure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import sklearn.metrics as metrics\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import math\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class MPNN(nn.Module):\n",
    "    '''\n",
    "    General base class for all varieties of Message Passing Neural Network.\n",
    "    '''\n",
    "    def __init__(self, n_msg_pass_iters, *args, **kwargs):\n",
    "        super(MPNN, self).__init__()\n",
    "        self.n_msg_pass_iters = n_msg_pass_iters\n",
    "    \n",
    "    def init_hidden_states_and_edges(self, graph):\n",
    "        # Performs \"init_hidden\" from above and prepares adjacency information from the graph\n",
    "        # (This function is here so the model can be flexible about what format the graph is given to us in.)\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def compute_messages(self, hidden_states, edges, t):\n",
    "        # Computes M_t from above and sums the messages\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def update_hidden_states(self, hidden_states, messages, t):\n",
    "        # Performs U_t from above\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def readout(self, hidden_states):\n",
    "        # Performs R_t from above\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def forward(self, graph):\n",
    "        hidden_states, edges = self.init_hidden_states_and_edges(graph)\n",
    "        for t in range(self.n_msg_pass_iters):\n",
    "            messages = self.compute_messages(hidden_states, edges, t)\n",
    "            hidden_states = self.update_hidden_states(hidden_states, messages, t)\n",
    "            \n",
    "        return self.readout(hidden_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different flavors of MPNN use different functions for $\\text{init_hidden}$, $M_t$, $U_t$, and $R_t$, and more often than not these functions are simpler than the fully general versions described above.  For example, in the GGSNN version of MPNN we'll discuss below, $M_t$ is the same function for each $t$, and it doesn't depend on the neighboring vertex's hidden state or any edge features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gated Graph Sequence Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got the MPNN framework down, let's grab some real data and implement a particular type of MPNN, called a Gated Graph Sequence Neural Network (GGSNN), to learn on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An actual dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a demonstration task, we'll use the [Tox21 dataset][1].  The objective of this dataset is to take in the [chemical structure of a molecule][2], represented as an undirected graph with atoms as vertices and bonds as edges, and predict the toxicity of the molecule.  In particular, we'll try to predict whether a molecule might [activate a particular cellular response to pollutants in your body][3].\n",
    "\n",
    "We'll create our own implementation to load and process the Tox21 dataset using RDKit for molecular feature extraction, eliminating the need for DeepChem.\n",
    "\n",
    "[1]:https://tripod.nih.gov/tox21/challenge/\n",
    "[2]:https://en.wikipedia.org/wiki/Structural_formula\n",
    "[3]:https://pubchem.ncbi.nlm.nih.gov/bioassay/743122#section=Top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll load the data and convert it to graph format using RDKit for molecular processing.  If you're not fluent in chemistry, don't worry about the details of the following preprocessing.  We're just transforming the data from a molecular format into the format we're used to seeing from above.\n",
    "\n",
    "What we'll end up with is a dataset of `(graph, label)` tuples where each `label` is a binary label (toxic or not), and each `graph` is an undirected graph represented as a vector of features for each vertex and an adjacency matrix.\n",
    "\n",
    "Our implementation will:\n",
    "1. Download the Tox21 dataset from a public source\n",
    "2. Convert SMILES strings to molecular graphs using RDKit\n",
    "3. Extract atom features and adjacency matrices\n",
    "4. Create train/validation/test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deepchem\n",
      "  Downloading deepchem-2.5.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: joblib in /u/shaswata/.conda/envs/pytorch_env/lib/python3.12/site-packages (from deepchem) (1.4.0)\n",
      "Requirement already satisfied: numpy in /u/shaswata/.conda/envs/pytorch_env/lib/python3.12/site-packages (from deepchem) (1.26.4)\n",
      "Requirement already satisfied: pandas in /u/shaswata/.conda/envs/pytorch_env/lib/python3.12/site-packages (from deepchem) (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in /u/shaswata/.conda/envs/pytorch_env/lib/python3.12/site-packages (from deepchem) (1.4.2)\n",
      "Requirement already satisfied: scipy in /u/shaswata/.conda/envs/pytorch_env/lib/python3.12/site-packages (from deepchem) (1.13.0)\n",
      "  Downloading deepchem-2.5.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: joblib in /u/shaswata/.conda/envs/pytorch_env/lib/python3.12/site-packages (from deepchem) (1.4.0)\n",
      "Requirement already satisfied: numpy in /u/shaswata/.conda/envs/pytorch_env/lib/python3.12/site-packages (from deepchem) (1.26.4)\n",
      "Requirement already satisfied: pandas in /u/shaswata/.conda/envs/pytorch_env/lib/python3.12/site-packages (from deepchem) (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in /u/shaswata/.conda/envs/pytorch_env/lib/python3.12/site-packages (from deepchem) (1.4.2)\n",
      "Requirement already satisfied: scipy in /u/shaswata/.conda/envs/pytorch_env/lib/python3.12/site-packages (from deepchem) (1.13.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /u/shaswata/.conda/envs/pytorch_env/lib/python3.12/site-packages (from pandas->deepchem) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /u/shaswata/.conda/envs/pytorch_env/lib/python3.12/site-packages (from pandas->deepchem) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /u/shaswata/.conda/envs/pytorch_env/lib/python3.12/site-packages (from pandas->deepchem) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /u/shaswata/.conda/envs/pytorch_env/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->deepchem) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /u/shaswata/.conda/envs/pytorch_env/lib/python3.12/site-packages (from scikit-learn->deepchem) (3.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /u/shaswata/.conda/envs/pytorch_env/lib/python3.12/site-packages (from pandas->deepchem) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /u/shaswata/.conda/envs/pytorch_env/lib/python3.12/site-packages (from pandas->deepchem) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /u/shaswata/.conda/envs/pytorch_env/lib/python3.12/site-packages (from pandas->deepchem) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /u/shaswata/.conda/envs/pytorch_env/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->deepchem) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /u/shaswata/.conda/envs/pytorch_env/lib/python3.12/site-packages (from scikit-learn->deepchem) (3.4.0)\n",
      "Downloading deepchem-2.5.0-py3-none-any.whl (552 kB)\n",
      "\u001b[?25l   \u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/552.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mDownloading deepchem-2.5.0-py3-none-any.whl (552 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m552.4/552.4 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m552.4/552.4 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: deepchem\n",
      "Installing collected packages: deepchem\n",
      "Successfully installed deepchem-2.5.0\n",
      "Successfully installed deepchem-2.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install RDKit for molecular processing\n",
    "!pip install rdkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:29:05] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset with 6549 molecules\n",
      "Train set: 4584 molecules\n",
      "Validation set: 982 molecules\n",
      "Test set: 983 molecules\n",
      "Training set class balance: 523/4584 positive samples\n"
     ]
    }
   ],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, rdMolDescriptors\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "def tox21_data():\n",
    "    \"\"\"Tox21 dataset from public source\"\"\"\n",
    "    filename = \"xAI_Drug/tox21.csv\"\n",
    "\n",
    "    return pd.read_csv(filename)\n",
    "\n",
    "def get_atom_features(atom):\n",
    "    \"\"\"Extract atom features for graph neural network\"\"\"\n",
    "    features = [\n",
    "        atom.GetAtomicNum(),\n",
    "        atom.GetDegree(),\n",
    "        atom.GetTotalDegree(),\n",
    "        atom.GetFormalCharge(),\n",
    "        int(atom.GetHybridization()),\n",
    "        int(atom.GetIsAromatic()),\n",
    "        int(atom.IsInRing()),\n",
    "        int(atom.IsInRingSize(3)),\n",
    "        int(atom.IsInRingSize(4)),\n",
    "        int(atom.IsInRingSize(5)),\n",
    "        int(atom.IsInRingSize(6)),\n",
    "        int(atom.IsInRingSize(7)),\n",
    "        int(atom.IsInRingSize(8)),\n",
    "    ]\n",
    "    \n",
    "    # One-hot encode atomic number for common elements\n",
    "    atomic_nums = [6, 7, 8, 9, 15, 16, 17, 35, 53]  # C, N, O, F, P, S, Cl, Br, I\n",
    "    for atomic_num in atomic_nums:\n",
    "        features.append(int(atom.GetAtomicNum() == atomic_num))\n",
    "    \n",
    "    # Pad to make feature vector length consistent\n",
    "    while len(features) < 75:\n",
    "        features.append(0)\n",
    "    \n",
    "    return features[:75]  # Ensure exactly 75 features\n",
    "\n",
    "def smiles_to_graph(smiles):\n",
    "    \"\"\"Convert SMILES string to graph representation\"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    \n",
    "    # Get atom features\n",
    "    atom_features = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        atom_features.append(get_atom_features(atom))\n",
    "    \n",
    "    # Get adjacency matrix\n",
    "    adj_mat = Chem.GetAdjacencyMatrix(mol)\n",
    "    \n",
    "    # Convert to sparse matrix\n",
    "    adj_sparse = sp.sparse.csr_matrix(adj_mat.astype(np.float32))\n",
    "    \n",
    "    return {\n",
    "        'vertex_features': np.array(atom_features, dtype=np.float32),\n",
    "        'adj_mat': adj_sparse\n",
    "    }\n",
    "\n",
    "def create_tox21_dataset():\n",
    "    \"\"\"Tox21 dataset from SMILES strings\"\"\"\n",
    "    # Download the data\n",
    "    df = tox21_data()\n",
    "    \n",
    "    # Focus on NR-AhR assay (nuclear receptor signaling bioassays)\n",
    "    target_column = 'NR-AhR'\n",
    "    \n",
    "    # Check if the column exists, if not use a different one\n",
    "    toxicity_columns = [col for col in df.columns if col.startswith('NR-') or col.startswith('SR-')]\n",
    "    if target_column not in df.columns and toxicity_columns:\n",
    "        target_column = toxicity_columns[0]\n",
    "        print(f\"Using {target_column} as target column\")\n",
    "    \n",
    "    \n",
    "    # Filter out rows with missing SMILES or target values\n",
    "    df = df.dropna(subset=['smiles', target_column])\n",
    "    \n",
    "    # Convert SMILES to graph representations\n",
    "    dataset = []\n",
    "    for idx, row in df.iterrows():\n",
    "        smiles = row['smiles']\n",
    "        label = int(row[target_column]) if not pd.isna(row[target_column]) else 0\n",
    "        \n",
    "        graph = smiles_to_graph(smiles)\n",
    "        if graph is not None:\n",
    "            graph['label'] = label\n",
    "            dataset.append(graph)\n",
    "    \n",
    "    print(f\"Created dataset with {len(dataset)} molecules\")\n",
    "    return dataset\n",
    "\n",
    "# Create the dataset\n",
    "full_dataset = create_tox21_dataset()\n",
    "\n",
    "# Split into train/validation/test sets\n",
    "train_dataset, temp_dataset = train_test_split(full_dataset, test_size=0.3, random_state=42)\n",
    "valid_dataset, test_dataset = train_test_split(temp_dataset, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Train set: {len(train_dataset)} molecules\")\n",
    "print(f\"Validation set: {len(valid_dataset)} molecules\")\n",
    "print(f\"Test set: {len(test_dataset)} molecules\")\n",
    "\n",
    "# Check class balance\n",
    "train_labels = [mol['label'] for mol in train_dataset]\n",
    "print(f\"Training set class balance: {sum(train_labels)}/{len(train_labels)} positive samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The GGSNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll implement a Gated Graph Sequence Neural Network, introduced in [this paper][1], on this dataset.\n",
    "\n",
    "A GGSNN is an MPNN with the following customizations:\n",
    "1. Hidden states $h_v^0$ for each vertex are initialized with a single-layer MLP.\n",
    "2. The messages passed to $v$ by its neighbors are a simple matrix multiplication of each neighbor's hidden state: $$m_v^{t+1} = \\sum_{w \\in \\text{neighbors of }v} W_{\\texttt{msg_fxn}}h_w^t.$$\n",
    "3. Each vertex $v$ updates its hidden state to be the output of a [GRU cell](https://pytorch.org/docs/stable/generated/torch.nn.GRUCell.html) (a type of RNN cell) whose hidden state is the vertex's hidden state and whose input is the message the vertex received: $$h_v^{t+1} = \\text{GRU}(m_v^{t+1}, h_v^t).$$\n",
    "4. The \"readout\" function is this funny little beast: $$\\hat{y} = \\text{softmax}\\left(f_{\\text{out}}\\left(\\sum_{v} \\sigma\\left(f_1([h_v^T, h_v^0])\\right) \\odot f_2(h_v^T)\\right)\\right),$$ where the $f$s are MLPs, $\\sigma$ is the sigmoid function, and $\\odot$ is elementwise multiplication.  This acts like a sort of attention mechanism that depends on how much each vertex's hidden state changed during message passing.\n",
    "\n",
    "Here's an implementation of GGSNN that fills out the details of the MPNN base class from above:\n",
    "\n",
    "> *A key implementation note about what follows:* You'll notice below that the GGSNN is coded as though it takes in a single graph, rather than a minibatch of graphs as you might expect.  This is intentional.  We want to reserve the 0th/batch dimension of the tensors in our implementation to index over the vertices of the graph.  This makes the implementation more elegant, since PyTorch operations are built to handle inputs that vary in size along the 0th dimension, and the number of vertices in each graph is usually different.\n",
    "\n",
    "> But of course, we DO want to process minibatches of data.  To do this, combine a minibatch of graphs into a single, large, disconnected graph, do all the message passing on this graph (no messages will get passed between minibatch elements, because their graphs are disconnected), and use the `batch_sizes` list to produce separate outputs for each graph in the minibatch in the `readout` step.\n",
    "\n",
    "[1]:https://arxiv.org/pdf/1511.05493.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GGSNN(MPNN):\n",
    "    '''\n",
    "    GGSNN model for operating on the Tox21 dataset\n",
    "    '''\n",
    "    def __init__(self, vertex_feature_size, hidden_size, output_size, **kwargs):\n",
    "        super(GGSNN, self).__init__(**kwargs)\n",
    "        \n",
    "        # Initializing model components\n",
    "        self.vertex_init = nn.Linear(vertex_feature_size, hidden_size)\n",
    "        self.message_fxn = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.gru = nn.GRUCell(hidden_size, hidden_size)\n",
    "        \n",
    "        # Readout networks\n",
    "        self.readout_1 = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size * 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size * 2, hidden_size)\n",
    "        )\n",
    "        self.readout_2 = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size)\n",
    "        )\n",
    "        self.readout_final = nn.Linear(hidden_size, output_size)\n",
    "                \n",
    "    def init_hidden_states_and_edges(self, graph):\n",
    "        # vertex_features are a (num_vertices x num_features) tensor \n",
    "        # edges is a (num_vertices x num_vertices) sparse tensor\n",
    "        # batch_sizes is a list of the sizes of the graphs in the batch that were combined into the graph\n",
    "        vertex_features, edges, batch_sizes = graph\n",
    "        init_hidden_states = torch.tanh(self.vertex_init(vertex_features))\n",
    "        # Saving these for use in the readout function later - not every MPNN requires this, but GGSNNs do\n",
    "        self.init_hidden_states = init_hidden_states.clone()\n",
    "        self.batch_sizes = batch_sizes.copy()\n",
    "        return init_hidden_states, edges\n",
    "    \n",
    "    def compute_messages(self, hidden_states, edges, t):\n",
    "        passed_msgs = self.message_fxn(hidden_states)\n",
    "        # For sparse matrix multiplication in PyTorch\n",
    "        summed_msgs = torch.sparse.mm(edges, passed_msgs)\n",
    "        return summed_msgs\n",
    "    \n",
    "    def update_hidden_states(self, hidden_states, messages, t):\n",
    "        hidden_states = self.gru(messages, hidden_states)\n",
    "        return hidden_states\n",
    "    \n",
    "    def readout(self, hidden_states):\n",
    "        readout_in_1 = torch.cat([hidden_states, self.init_hidden_states], dim=1)\n",
    "        readout_hid_1 = torch.sigmoid(self.readout_1(readout_in_1))\n",
    "        readout_hid_2 = self.readout_2(hidden_states)\n",
    "        readout_hid = readout_hid_1 * readout_hid_2\n",
    "        readout_attention = []\n",
    "        i = j = 0\n",
    "        while self.batch_sizes:\n",
    "            i = j\n",
    "            j += self.batch_sizes.pop(0)\n",
    "            readout_attention.append(torch.sum(readout_hid[i:j], dim=0, keepdim=True))\n",
    "        readout_attention = torch.cat(readout_attention, dim=0)\n",
    "        return self.readout_final(readout_attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's train!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a new GGSNN instance and initialize our model parameters, loss function, and optimizer as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = GGSNN(vertex_feature_size=75, hidden_size=100, output_size=2, n_msg_pass_iters=6)\n",
    "model.to(device)\n",
    "# Initialize weights\n",
    "for param in model.parameters():\n",
    "    if param.dim() > 1:\n",
    "        nn.init.normal_(param, std=0.01)\n",
    "    else:\n",
    "        nn.init.constant_(param, 0)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll add a few helper functions to keep the training loop code clean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batchify_graphs(graphs):\n",
    "    '''\n",
    "    Args:\n",
    "        batch: List of graphs in {vertex_feature, adjacency_matrix, label} format\n",
    "        \n",
    "    Returns:\n",
    "        The combination of the input graphs into a big disconnected graph\n",
    "        The labels of each of the input graphs\n",
    "    '''\n",
    "    vertex_features = np.concatenate([g['vertex_features'] for g in graphs])\n",
    "    vertex_features = torch.tensor(vertex_features, dtype=torch.float32, device=device)\n",
    "    \n",
    "    # Create block diagonal sparse matrix for adjacency\n",
    "    adj_mat = sp.sparse.block_diag([g['adj_mat'] for g in graphs]).tocoo()\n",
    "    indices = torch.tensor(np.vstack([adj_mat.row, adj_mat.col]), dtype=torch.long, device=device)\n",
    "    values = torch.tensor(adj_mat.data, dtype=torch.float32, device=device)\n",
    "    adj_mat = torch.sparse_coo_tensor(indices, values, adj_mat.shape).to_sparse_csr()\n",
    "    \n",
    "    batch_sizes = [g['vertex_features'].shape[0] for g in graphs]\n",
    "    labels = torch.tensor([g['label'] for g in graphs], dtype=torch.long, device=device)\n",
    "    return (vertex_features, adj_mat, batch_sizes), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(dataset, model, n_batch):\n",
    "    '''\n",
    "    Measures the accuracy of the model on the provided dataset, in batches\n",
    "    '''\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, math.ceil(len(dataset)/n_batch)):\n",
    "            data = dataset[n_batch*i:n_batch*(i+1)]\n",
    "            graph, label = batchify_graphs(data)\n",
    "            output = model(graph)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += label.size(0)\n",
    "            correct += (predicted == label).sum().item()\n",
    "    \n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_roc_score(dataset, model, n_batch):\n",
    "    '''\n",
    "    Measures the area under the ROC curve of the model on the provided dataset, in batches\n",
    "    '''\n",
    "    model.eval()\n",
    "    pos_probs = []\n",
    "    labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, math.ceil(len(dataset)/n_batch)):\n",
    "            data = dataset[n_batch*i:n_batch*(i+1)]\n",
    "            graph, label = batchify_graphs(data)\n",
    "            output = model(graph)\n",
    "            probs = F.softmax(output, dim=1)\n",
    "            pos_probs.append(probs[:, 1])\n",
    "            labels.append(label)\n",
    "    \n",
    "    labels = torch.cat(labels, dim=0).cpu().numpy()\n",
    "    pos_probs = torch.cat(pos_probs, dim=0).cpu().numpy()\n",
    "    return metrics.roc_auc_score(labels, pos_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the class balance in the dataset is heavily skewed toward the \"not toxic\" label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of \"not toxic\" labels in training data = 0.8859075043630017\n"
     ]
    }
   ],
   "source": [
    "labels = np.array([i['label'] for i in train_dataset])\n",
    "print('Percentage of \"not toxic\" labels in training data = {}'.format(sum(labels == 0)/len(labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is why in the training loop below we're measuring the [ROC AUC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic), in addition to just the accuracy.\n",
    "\n",
    "Now let's train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1683755/571810304.py:17: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at ../aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "  adj_mat = torch.sparse_coo_tensor(indices, values, adj_mat.shape).to_sparse_csr()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 0.6012, \n",
      "\tTrain_acc 0.8859, Valid_acc 0.8768\n",
      "\tTrain_roc_auc 0.3968, Valid_roc_auc 0.4038\n",
      "Epoch 1. Loss: 0.4381, \n",
      "\tTrain_acc 0.8859, Valid_acc 0.8768\n",
      "\tTrain_roc_auc 0.4104, Valid_roc_auc 0.4195\n",
      "Epoch 1. Loss: 0.4381, \n",
      "\tTrain_acc 0.8859, Valid_acc 0.8768\n",
      "\tTrain_roc_auc 0.4104, Valid_roc_auc 0.4195\n",
      "Epoch 2. Loss: 0.4287, \n",
      "\tTrain_acc 0.8859, Valid_acc 0.8768\n",
      "\tTrain_roc_auc 0.4228, Valid_roc_auc 0.4339\n",
      "Epoch 2. Loss: 0.4287, \n",
      "\tTrain_acc 0.8859, Valid_acc 0.8768\n",
      "\tTrain_roc_auc 0.4228, Valid_roc_auc 0.4339\n",
      "Epoch 3. Loss: 0.4212, \n",
      "\tTrain_acc 0.8859, Valid_acc 0.8768\n",
      "\tTrain_roc_auc 0.4385, Valid_roc_auc 0.4515\n",
      "Epoch 3. Loss: 0.4212, \n",
      "\tTrain_acc 0.8859, Valid_acc 0.8768\n",
      "\tTrain_roc_auc 0.4385, Valid_roc_auc 0.4515\n",
      "Epoch 4. Loss: 0.4132, \n",
      "\tTrain_acc 0.8859, Valid_acc 0.8768\n",
      "\tTrain_roc_auc 0.4618, Valid_roc_auc 0.4782\n",
      "Epoch 4. Loss: 0.4132, \n",
      "\tTrain_acc 0.8859, Valid_acc 0.8768\n",
      "\tTrain_roc_auc 0.4618, Valid_roc_auc 0.4782\n",
      "Epoch 5. Loss: 0.4024, \n",
      "\tTrain_acc 0.8859, Valid_acc 0.8768\n",
      "\tTrain_roc_auc 0.5075, Valid_roc_auc 0.5282\n",
      "Epoch 5. Loss: 0.4024, \n",
      "\tTrain_acc 0.8859, Valid_acc 0.8768\n",
      "\tTrain_roc_auc 0.5075, Valid_roc_auc 0.5282\n",
      "Epoch 6. Loss: 0.3837, \n",
      "\tTrain_acc 0.8859, Valid_acc 0.8768\n",
      "\tTrain_roc_auc 0.6399, Valid_roc_auc 0.6711\n",
      "Epoch 6. Loss: 0.3837, \n",
      "\tTrain_acc 0.8859, Valid_acc 0.8768\n",
      "\tTrain_roc_auc 0.6399, Valid_roc_auc 0.6711\n",
      "Epoch 7. Loss: 0.3561, \n",
      "\tTrain_acc 0.8866, Valid_acc 0.8768\n",
      "\tTrain_roc_auc 0.6703, Valid_roc_auc 0.6977\n",
      "Epoch 7. Loss: 0.3561, \n",
      "\tTrain_acc 0.8866, Valid_acc 0.8768\n",
      "\tTrain_roc_auc 0.6703, Valid_roc_auc 0.6977\n",
      "Epoch 8. Loss: 0.3535, \n",
      "\tTrain_acc 0.8872, Valid_acc 0.8870\n",
      "\tTrain_roc_auc 0.6985, Valid_roc_auc 0.7232\n",
      "Epoch 8. Loss: 0.3535, \n",
      "\tTrain_acc 0.8872, Valid_acc 0.8870\n",
      "\tTrain_roc_auc 0.6985, Valid_roc_auc 0.7232\n",
      "Epoch 9. Loss: 0.3507, \n",
      "\tTrain_acc 0.8870, Valid_acc 0.8809\n",
      "\tTrain_roc_auc 0.6918, Valid_roc_auc 0.7178\n",
      "Epoch 9. Loss: 0.3507, \n",
      "\tTrain_acc 0.8870, Valid_acc 0.8809\n",
      "\tTrain_roc_auc 0.6918, Valid_roc_auc 0.7178\n",
      "Epoch 10. Loss: 0.3487, \n",
      "\tTrain_acc 0.8866, Valid_acc 0.8859\n",
      "\tTrain_roc_auc 0.6988, Valid_roc_auc 0.7239\n",
      "Epoch 10. Loss: 0.3487, \n",
      "\tTrain_acc 0.8866, Valid_acc 0.8859\n",
      "\tTrain_roc_auc 0.6988, Valid_roc_auc 0.7239\n",
      "Epoch 11. Loss: 0.3464, \n",
      "\tTrain_acc 0.8863, Valid_acc 0.8870\n",
      "\tTrain_roc_auc 0.7011, Valid_roc_auc 0.7261\n",
      "Epoch 11. Loss: 0.3464, \n",
      "\tTrain_acc 0.8863, Valid_acc 0.8870\n",
      "\tTrain_roc_auc 0.7011, Valid_roc_auc 0.7261\n",
      "Epoch 12. Loss: 0.3449, \n",
      "\tTrain_acc 0.8861, Valid_acc 0.8870\n",
      "\tTrain_roc_auc 0.7031, Valid_roc_auc 0.7280\n",
      "Epoch 12. Loss: 0.3449, \n",
      "\tTrain_acc 0.8861, Valid_acc 0.8870\n",
      "\tTrain_roc_auc 0.7031, Valid_roc_auc 0.7280\n",
      "Epoch 13. Loss: 0.3438, \n",
      "\tTrain_acc 0.8861, Valid_acc 0.8870\n",
      "\tTrain_roc_auc 0.7047, Valid_roc_auc 0.7291\n",
      "Epoch 13. Loss: 0.3438, \n",
      "\tTrain_acc 0.8861, Valid_acc 0.8870\n",
      "\tTrain_roc_auc 0.7047, Valid_roc_auc 0.7291\n",
      "Epoch 14. Loss: 0.3428, \n",
      "\tTrain_acc 0.8861, Valid_acc 0.8870\n",
      "\tTrain_roc_auc 0.7061, Valid_roc_auc 0.7303\n",
      "Epoch 14. Loss: 0.3428, \n",
      "\tTrain_acc 0.8861, Valid_acc 0.8870\n",
      "\tTrain_roc_auc 0.7061, Valid_roc_auc 0.7303\n",
      "Epoch 15. Loss: 0.3419, \n",
      "\tTrain_acc 0.8863, Valid_acc 0.8870\n",
      "\tTrain_roc_auc 0.7075, Valid_roc_auc 0.7312\n",
      "Epoch 15. Loss: 0.3419, \n",
      "\tTrain_acc 0.8863, Valid_acc 0.8870\n",
      "\tTrain_roc_auc 0.7075, Valid_roc_auc 0.7312\n",
      "Epoch 16. Loss: 0.3411, \n",
      "\tTrain_acc 0.8866, Valid_acc 0.8870\n",
      "\tTrain_roc_auc 0.7088, Valid_roc_auc 0.7324\n",
      "Epoch 16. Loss: 0.3411, \n",
      "\tTrain_acc 0.8866, Valid_acc 0.8870\n",
      "\tTrain_roc_auc 0.7088, Valid_roc_auc 0.7324\n",
      "Epoch 17. Loss: 0.3403, \n",
      "\tTrain_acc 0.8866, Valid_acc 0.8870\n",
      "\tTrain_roc_auc 0.7101, Valid_roc_auc 0.7334\n",
      "Epoch 17. Loss: 0.3403, \n",
      "\tTrain_acc 0.8866, Valid_acc 0.8870\n",
      "\tTrain_roc_auc 0.7101, Valid_roc_auc 0.7334\n",
      "Epoch 18. Loss: 0.3395, \n",
      "\tTrain_acc 0.8866, Valid_acc 0.8859\n",
      "\tTrain_roc_auc 0.7113, Valid_roc_auc 0.7346\n",
      "Epoch 18. Loss: 0.3395, \n",
      "\tTrain_acc 0.8866, Valid_acc 0.8859\n",
      "\tTrain_roc_auc 0.7113, Valid_roc_auc 0.7346\n",
      "Epoch 19. Loss: 0.3388, \n",
      "\tTrain_acc 0.8868, Valid_acc 0.8849\n",
      "\tTrain_roc_auc 0.7124, Valid_roc_auc 0.7357\n",
      "Epoch 19. Loss: 0.3388, \n",
      "\tTrain_acc 0.8868, Valid_acc 0.8849\n",
      "\tTrain_roc_auc 0.7124, Valid_roc_auc 0.7357\n",
      "Epoch 20. Loss: 0.3380, \n",
      "\tTrain_acc 0.8868, Valid_acc 0.8849\n",
      "\tTrain_roc_auc 0.7136, Valid_roc_auc 0.7370\n",
      "Epoch 20. Loss: 0.3380, \n",
      "\tTrain_acc 0.8868, Valid_acc 0.8849\n",
      "\tTrain_roc_auc 0.7136, Valid_roc_auc 0.7370\n",
      "Epoch 21. Loss: 0.3374, \n",
      "\tTrain_acc 0.8868, Valid_acc 0.8849\n",
      "\tTrain_roc_auc 0.7148, Valid_roc_auc 0.7378\n",
      "Epoch 21. Loss: 0.3374, \n",
      "\tTrain_acc 0.8868, Valid_acc 0.8849\n",
      "\tTrain_roc_auc 0.7148, Valid_roc_auc 0.7378\n",
      "Epoch 22. Loss: 0.3367, \n",
      "\tTrain_acc 0.8866, Valid_acc 0.8849\n",
      "\tTrain_roc_auc 0.7160, Valid_roc_auc 0.7391\n",
      "Epoch 22. Loss: 0.3367, \n",
      "\tTrain_acc 0.8866, Valid_acc 0.8849\n",
      "\tTrain_roc_auc 0.7160, Valid_roc_auc 0.7391\n",
      "Epoch 23. Loss: 0.3360, \n",
      "\tTrain_acc 0.8868, Valid_acc 0.8849\n",
      "\tTrain_roc_auc 0.7172, Valid_roc_auc 0.7401\n",
      "Epoch 23. Loss: 0.3360, \n",
      "\tTrain_acc 0.8868, Valid_acc 0.8849\n",
      "\tTrain_roc_auc 0.7172, Valid_roc_auc 0.7401\n",
      "Epoch 24. Loss: 0.3353, \n",
      "\tTrain_acc 0.8868, Valid_acc 0.8849\n",
      "\tTrain_roc_auc 0.7183, Valid_roc_auc 0.7414\n",
      "Epoch 24. Loss: 0.3353, \n",
      "\tTrain_acc 0.8868, Valid_acc 0.8849\n",
      "\tTrain_roc_auc 0.7183, Valid_roc_auc 0.7414\n",
      "Epoch 25. Loss: 0.3347, \n",
      "\tTrain_acc 0.8866, Valid_acc 0.8849\n",
      "\tTrain_roc_auc 0.7196, Valid_roc_auc 0.7423\n",
      "Epoch 25. Loss: 0.3347, \n",
      "\tTrain_acc 0.8866, Valid_acc 0.8849\n",
      "\tTrain_roc_auc 0.7196, Valid_roc_auc 0.7423\n",
      "Epoch 26. Loss: 0.3341, \n",
      "\tTrain_acc 0.8866, Valid_acc 0.8839\n",
      "\tTrain_roc_auc 0.7208, Valid_roc_auc 0.7434\n",
      "Epoch 26. Loss: 0.3341, \n",
      "\tTrain_acc 0.8866, Valid_acc 0.8839\n",
      "\tTrain_roc_auc 0.7208, Valid_roc_auc 0.7434\n",
      "Epoch 27. Loss: 0.3334, \n",
      "\tTrain_acc 0.8866, Valid_acc 0.8839\n",
      "\tTrain_roc_auc 0.7221, Valid_roc_auc 0.7443\n",
      "Epoch 27. Loss: 0.3334, \n",
      "\tTrain_acc 0.8866, Valid_acc 0.8839\n",
      "\tTrain_roc_auc 0.7221, Valid_roc_auc 0.7443\n",
      "Epoch 28. Loss: 0.3328, \n",
      "\tTrain_acc 0.8870, Valid_acc 0.8829\n",
      "\tTrain_roc_auc 0.7234, Valid_roc_auc 0.7456\n",
      "Epoch 28. Loss: 0.3328, \n",
      "\tTrain_acc 0.8870, Valid_acc 0.8829\n",
      "\tTrain_roc_auc 0.7234, Valid_roc_auc 0.7456\n",
      "Epoch 29. Loss: 0.3322, \n",
      "\tTrain_acc 0.8870, Valid_acc 0.8819\n",
      "\tTrain_roc_auc 0.7247, Valid_roc_auc 0.7466\n",
      "Test Accuracy: 0.8749\n",
      "Epoch 29. Loss: 0.3322, \n",
      "\tTrain_acc 0.8870, Valid_acc 0.8819\n",
      "\tTrain_roc_auc 0.7247, Valid_roc_auc 0.7466\n",
      "Test Accuracy: 0.8749\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 30\n",
    "n_batch = 128\n",
    "\n",
    "for e in range(n_epochs):\n",
    "    model.train()\n",
    "    cumulative_loss = 0\n",
    "    \n",
    "    for i in range(0, math.ceil(len(train_dataset)/n_batch)):\n",
    "        data = train_dataset[n_batch*i:n_batch*(i+1)]\n",
    "        graph, label = batchify_graphs(data)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(graph)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        cumulative_loss += loss.item() * len(data)\n",
    "    \n",
    "    valid_accuracy = evaluate_accuracy(valid_dataset, model, n_batch)\n",
    "    train_accuracy = evaluate_accuracy(train_dataset, model, n_batch)\n",
    "    valid_roc = evaluate_roc_score(valid_dataset, model, n_batch)\n",
    "    train_roc = evaluate_roc_score(train_dataset, model, n_batch)\n",
    "    \n",
    "    print('Epoch {}. Loss: {:.4f}, \\n\\tTrain_acc {:.4f}, Valid_acc {:.4f}\\n\\tTrain_roc_auc {:.4f}, Valid_roc_auc {:.4f}'.format(\n",
    "            e, cumulative_loss/len(train_dataset), train_accuracy, valid_accuracy, train_roc, valid_roc))\n",
    "    \n",
    "print('Test Accuracy: {:.4f}'.format(evaluate_accuracy(test_dataset, model, n_batch)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright!  Unsurprisingly, given the class imbalance, our accuracy didn't improve much; but our ROC score got much better, in line with the current state of the art on this dataset: see the physiology section [here](http://moleculenet.ai/latest-results).\n",
    "\n",
    "Now go forth and invent your own types of MPNNs!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
